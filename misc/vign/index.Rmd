---
title: "rdwd"
author: "Berry Boessenkool, <berry-b@gmx.de>"
date: "build `r Sys.Date()` with rdwd version `r packageVersion('rdwd')` and dwdradar version `r packageVersion('dwdradar')`"
site: bookdown::bookdown_site
documentclass: book
output:
  bookdown::gitbook: default
editor_options: 
  chunk_output_type: console
---

# Intro

`rdwd` is an R package to handle data from the German Weather Service (DWD).

This vignette has 3 main sections:

* time series from meteorological stations (chapters 2-5)
* raster data from radar + interpolation (chapter 6)
* use cases, i.e. extended usage examples (chapter 7-end)

Important links:

* further details on the data: [DWD FTP server documentation](ftp://opendata.dwd.de/climate_environment/CDC/Readme_intro_CDC_ftp.pdf)
* vignette [source code and files](https://github.com/brry/rdwd/tree/master/misc/vign)
* feedback is very welcome via [github](https://github.com/brry/rdwd) or <berry-b@gmx.de>!  


```{r globalquiet, echo=FALSE}
options(rdwdquiet=TRUE)
# This should suppress all progress and info messages in the vignette output
# while not suggesting users to copypaste with explicit quiet=TRUE all the time.
# Especially new users should see the messages and progress bars, I think
```


*The remainder of this intro chapter is a copy of the [github README file](https://github.com/brry/rdwd#rdwd).*

----

```{r readmecontent, echo=FALSE, message=FALSE, results='asis'}
# mirror section of README
reme <- readLines('../../README.md')
doc <- grep("## Documentation", reme) + 0:4
reme <- gsub("###", "##", reme)
cat(reme[-c(1,2,doc)], sep='\n')
```

```{r helplink_macro, echo=FALSE}
helplink <- function(doc) paste0("[`",doc,"`](https://www.rdocumentation.org/packages/rdwd/topics/",doc,")")
```

```{r lib_rdwd, echo=FALSE}
library(rdwd)
print_short <- function(x) 
  {out <- lapply(x, gsub, pattern=dwdbase, replacement="---")
  if(is.list(x)) out else unlist(out)}
```



# Interactive map

The `rdwd` package provides a collection of all the metafiles on the 
[DWD data server](ftp://opendata.dwd.de/climate_environment/CDC/observations_germany/climate).
It is presented as an interactive map below.

When a point is clicked, an infobox should appear.
The first line can be copypasted into R to obtain more information on the available files.
The map is created with the following code:

```{r map, fig.height=7, fig.width=7, warning=FALSE, screenshot.force=FALSE}
library(rdwd)  ;  data(geoIndex)  ;  library(leaflet) 
leaflet(geoIndex) %>% addTiles() %>%
        addCircles(~lon, ~lat, radius=900, stroke=F, color=~col) %>%
        addCircleMarkers(~lon, ~lat, popup=~display, stroke=F, color=~col)
```

The blue dots mark stations for which recent files are available 
(with >=1 file in 'recent' folder or 'BIS_DATUM' later than one year ago).
The red dots mark all stations with only historical datasets.

To see only the stations with recent data, use the following code:

```{r onlyrecent, eval=FALSE}
library(rdwd)  ;  data(geoIndex)  ;  library(leaflet) 
leaflet(data=geoIndex[geoIndex$recentfile,]) %>% addTiles() %>%
        addCircleMarkers(~lon, ~lat, popup=~display, stroke=F)
```

To request the nonpublic datasets counted in the infobox, please contact <cdc.daten@dwd.de> or <klima.vertrieb@dwd.de>.
(The DWD cannot publish all datasets because of copyright restrictions).

Note: `r helplink("geoIndex")` is created using `r helplink("createIndex")` in
[`updateIndexes`](https://github.com/brry/rdwd/blob/master/R/updateIndexes.R).



# Available datasets

*overview of the FTP folder structure*

The following folders in **`res/var/per`** notation (resolution/variable/period) are available at
[`dwdbase`](ftp://opendata.dwd.de/climate_environment/CDC/observations_germany/climate).

Here's a full example URL, for a file at hourly/wind/recent:
[ftp://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/](ftp://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/wind/recent/stundenwerte_FF_00164_akt.zip)
[hourly/wind/recent/stundenwerte_FF_00164_akt.zip](ftp://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/wind/recent/stundenwerte_FF_00164_akt.zip)

"<" signifies a split into the folders `per` = "recent" and "historical".  
"<<" signifies a split into the folders `per` = "now", "recent", "historical" and "meta_data".  
"-" signifies that there are no further sub-folders.  
The symbols are clickable URLS leading to the corresponding **`res/var/`** FTP folder.  

Please note:

* both "solar" (-/<<) and "sun" (<) are available!  
* `1_minute/precipitation/historical` has a subfolder for each year.  
* `subdaily/standard_format` does not actually split in subfolders, but has recent/hist information in `fileIndex`

```{r ftp_folders_1, echo=FALSE}
library(magrittr)
suppressPackageStartupMessages(library(huxtable))
f <- readODS::read_ods("FTP_folders.ods", sheet="folders", col_types=NA)
fu <- f # f with urls
for(i in 2:ncol(f)) fu[-1,i] <- ifelse(f[-1,i]=="", " ", 
    paste0('<a href="',dwdbase,'/',colnames(f)[i],'/',f[-1,1],'">',f[-1,i],'</a>'))
ht <- huxtable::hux(fu, add_colnames=TRUE)
ht %>%
  set_background_color(value="white") %>% 
  set_background_color(3:nrow(ht), 2:ncol(ht), value="gray95") %>% 
  set_all_borders(1) %>%
  # set_bottom_border(1, everywhere, 2) %>%
  set_bold(1, -1, TRUE) %>% 
  set_rotation(1, -1, 90) %>%
  set_align(everywhere, -1, "center") %>% 
  set_valign(row=1, col=1, "bottom") %>% 
  map_background_color(by_values(" "="white")) %>% 
  set_position("center") %>% 
  set_escape_contents(value=FALSE)
detach("package:huxtable", unload=TRUE) # to suppress further autoformatting of tables
```

```{r ftp_folders_missing, echo=FALSE}
library(rdwd)
data("fileIndex")
# online and table folder comparison
on <- paste(fileIndex$res, fileIndex$var, fileIndex$per, sep="/")
on <- unique(on)

tab <- lapply(colnames(f)[-1], function(r) 
  {
  if(r=="multi_annual") return(paste0(r,"//",f[f[,r]=="-",1]))
  per4 <- c("now", "recent", "historical", "meta_data")
  per2 <- c("recent", "historical")
  r0 <- f[,r]=="-"
  r2 <- f[,r]=="<"
  r4 <- f[,r]=="<<"
  rv0 <- paste0(r,"/", f[r0, 1], "/")
  rv2 <- paste0(r,"/", f[r2, 1], "/")
  rv4 <- paste0(r,"/", f[r4, 1], "/")
  rv0 <- if(any(r0)) rv0 else ""[0]
  rv2 <- if(any(r2)) paste0(rep(rv2, each=length(per2)), per2) else ""[0]
  rv4 <- if(any(r4)) paste0(rep(rv4, each=length(per4)), per4) else ""[0]
  c(rv0,rv2,rv4)
  })
tab <- unlist(tab)

miss <- on[!on %in% tab]
miss <- miss[! miss %in% c("subdaily/standard_format/recent", "subdaily/standard_format/historical")]
l <- length(miss)
if(l>0) stop("\n\nThe following ", l, " FTP folders must yet be added to the ",
             "table (FTP_folder.ods) in section 'Available datasets':\n", 
             paste(miss, collapse="\n"))
miss <- tab[!tab %in% on]
l <- length(miss)
if(l>0) stop("\n\nThe following ", l, " table entries in section 'Available datasets' (from FTP_folder.ods)",
             " are no longer in the FTP folder:\n", paste(miss, collapse="\n"))
```


# Package structure

To use the observational datasets, `rdwd` has been designed to mainly do 3 things:

* `r helplink("selectDWD")`: facilitate file selection, needing:
  * station id/name/location (see chapter [station selection](station-selection.html))
  * temporal resolution (**res** = 1/10 minutes, hourly, daily, monthly, annual)
  * variables (**var** = temperature, rain, wind, sun, clouds, etc)
  * observation period (**per** = historical long term records or the current year)  


* `r helplink("dataDWD")`: download a file (or multiple files), without re-downloads for existing files

* `r helplink("readDWD")`: read that data into R (including useful defaults for metadata)

`r helplink("selectDWD")` uses the result from `r helplink("indexFTP")` which recursively lists all the files on an FTP-server (using `RCurl::getURL`).
As this is time consuming, the result is stored in the package dataset `r helplink("fileIndex")`.
From this, `r helplink("metaIndex")` and `r helplink("geoIndex")` are derived.

<img src="PackageSchematic.png" width="600">



# Station selection

*`print_short` in this chapter is just a helper function to replace [dwdbase](ftp://opendata.dwd.de/climate_environment/CDC/observations_germany/climate) with `---` for shorter output in the vignette.*

## by location

Weather stations can be selected geographically with the [interactive map](interactive-map.html).
All stations within a certain radius around a given lat-long position can be obtained with
`r helplink("nearbyStations")`.

## by ID

The DWD station IDs can be obtained from station names with `r helplink("findID")`:
```{r findID, eval=TRUE}
findID("Potsdam")
findID("Koeln", exactmatch=FALSE)
```

## by name

File selection by station name/id and folder can happen directly with `r helplink("selectDWD")`.
It is designed to be very flexible:

```{r select1, eval=FALSE, echo=TRUE}
# inputs can be vectorized, and period can be abbreviated:
selectDWD(c("Potsdam","Wuerzburg"), res="hourly", var="sun", per="hist") %>% print_short
```

```{r select2, eval=TRUE, echo=FALSE}
selectDWD(c("Potsdam","Wuerzburg"), res="hourly", var="sun", per="hist") %>% print_short
```
If res/var/per are left NA, an interactive selection is opened with the 
[available folder options](available-datasets.html) for the given station.

The time period can be doubled to get both filenames:
```{r select3}
selectDWD("Potsdam", res="daily", var="kl", per="rh") %>% print_short
```

There may be a differing number of available files for several stations across all folders.
That's why the default outvec is FALSE (unless `per="hr"`).

```{r select5, eval=TRUE}
selectDWD(id=c(3467,5116), res="",var="",per="") %>% print_short
```



## fileIndex

`r helplink("selectDWD")` needs an index of all the available files on the server.
The package contains such an index (`r helplink("fileIndex")`) that is updated (at least) with each CRAN release of the package.
If you find the file index to be outdated (Error in download.file ... : cannot open URL),
please let me know and I will update it. You might also want to check if the github development version (`r helplink("updateRdwd")`) already has an updated index. Meanwhile, use current=TRUE in `r helplink("selectDWD")`:

```{r files, eval=FALSE}
# all files at a given path, with current file index (RCurl required):
links <- selectDWD(res="monthly", var="more_precip", per="hist", current=TRUE)
```

Alternatively, for repetitive usage, create your own file index.  
`r helplink("fileIndex")` for the entire DWD FTP server is created with the functions
`r helplink("indexFTP")` and `r helplink("createIndex")` used in
[`updateIndexes`](https://github.com/brry/rdwd/blob/master/R/updateIndexes.R).  
To get an index for a certain subfolder, use:

```{r listfiles, eval=FALSE}
### This chunk is not evaluated ###
# recursively list files on the FTP-server:
files <- indexFTP("hourly/sun") # use dir="some_path" to save the output elsewhere
berryFunctions::headtail(files, 5, na=TRUE)

# indexFTP uses a folder to resume indexing after getting banned:
radfiles <- indexFTP("hourly/radolan/recent", gridbase)
radfiles <- indexFTP(radfiles,gridbase, sleep=1)

# with other FTP servers, this should also work...
funet <- indexFTP(base="ftp.funet.fi/pub/standards/w3/TR/xhtml11/", folder="")
p <- RCurl::getURL(    "ftp.funet.fi/pub/standards/w3/TR/xhtml11/",
                       verbose=T, ftp.use.epsv=TRUE, dirlistonly=TRUE)
```


## metaIndex

`r helplink("selectDWD")` also uses a complete data.frame with meta information,
`r helplink("metaIndex")` 
(derived from the "Beschreibung" files in `r helplink("fileIndex")`).
```{r meta23, eval=TRUE}
# All metadata at all folders:
data(metaIndex)
str(metaIndex, vec.len=2)
```
```{r metaView, eval=FALSE}
View(data.frame(sort(unique(rdwd:::metaIndex$Stationsname)))) # ca 6k entries
```

`r helplink("dataDWD")` can download (and `r helplink("readDWD")` can correctly read) such a data.frame from any folder on the FTP server:
```{r meta1, eval=TRUE}
# file with station metadata for a given path:
m_link <- selectDWD(res="monthly", var="more_precip", per="hist", meta=TRUE)
print_short(m_link) # (Monatswerte = monthly values, Beschreibung = description)
```
```{r meta2, eval=FALSE}
meta_monthly_rain <- dataDWD(m_link) # not executed in vignette creation
str(meta_monthly_rain)
```

Meta files may list stations for which there are actually no files.
These refer to nonpublic datasets (The DWD cannot publish all datasets because of copyright restrictions).
To request those, please contact <cdc.daten@dwd.de> or <klima.vertrieb@dwd.de>.



# Raster data

For observational data at `r helplink("dwdbase")`, 
`r helplink("selectDWD")` is the main function to choose data to be downloaded.

For gridded data at [`gridbase`](ftp://opendata.dwd.de/climate_environment/CDC/grids_germany), including 
data interpolated onto a 1 km raster and radar data up to the last hour, 
I don't yet understand the structure of the FTP server as well.  
For now, you'll have to query `r helplink("gridIndex")` yourself, e.g. with
```{r, gridIndex, eval=FALSE}
data(gridIndex)
head(grep("historical", gridIndex, value=TRUE))

# currently available files in a given folder:
rasterbase <- paste0(gridbase,"/seasonal/air_temperature_mean")
ftp.files <- indexFTP("/16_DJF", base=rasterbase, dir=tempdir())

# current index of all grid files (takes > 2 min, yields >30k charstrings >5MB):
gridIndexNow <- indexFTP(folder="currentgindex", base=gridbase, filename="grids")
```
If you send me examples of how you use it, I can then expand this in `rdwd`.  
For files that are not yet read correctly, you can also consult the 
Kompositformatbeschreibung at <https://www.dwd.de/DE/leistungen/radolan/radolan.html>

Besides `r helplink("dwdbase")` and [`gridbase`](ftp://opendata.dwd.de/climate_environment/CDC/grids_germany), 
there's yet more data at <ftp://ftp-cdc.dwd.de/weather>.

A helper function to reduce code duplication:

```{r readDWD_gridded}
ddir <- localtestdir()
tdir <- tempdir()
project_and_plot <- function(x, main1, main2, main3=NULL, ...)
  {
  par(mar=c(2,2,2.2,5), mgp=c(3,0.7,0))
  main <- paste(main1, "\n", as.character(main2), as.character(main3))
  out <- plotRadar(x, main=main, layer=1, ...)
  return(invisible(c(pp,out)))
  }
pp <- list()
```

The following overview will usually unzip only a few selected files for speed and memory considerations.
In real life, you probably do not want to unzip to a temporary `exdir`.
You can also remove `read=FALSE` in dataDWD and add the needed arguments right there, 
but I wanted to be explicit here.

The first line in each code block shows for which FTP folder 
at [`gridbase`](ftp://opendata.dwd.de/climate_environment/CDC/grids_germany) 
this function will be called.  
The last line shows what projection and extent to use in `r helplink("projectRasterDWD")`. 

## readDWD.raster
`r helplink("readDWD.raster")`  
```{r readDWD_raster}
link <- "seasonal/air_temperature_mean/16_DJF/grids_germany_seasonal_air_temp_mean_188216.asc.gz" # 0.2 MB
file <- dataDWD(link, base=gridbase, joinbf=TRUE, dir=ddir, read=FALSE)
rad <- readDWD(file) # with dividebyten=TRUE
rad <- readDWD(file) # runs faster at second time due to skip=TRUE
pp <- project_and_plot(rad, ".raster", "", proj="seasonal", extent=rad@extent)
```

## readDWD.nc
`r helplink("readDWD.nc")`
```{r readDWD_nc}
link <- "daily/Project_TRY/pressure/PRED_199606_daymean.nc.gz"  #  5 MB
file <- dataDWD(link, base=gridbase, joinbf=TRUE, dir=ddir, read=FALSE)
rad <- readDWD(file) # can also have interactive selection of variable
pp <- project_and_plot(rad, ".nc", rad@title, rad@z[[1]][1], proj="nc", extent="nc")
```

## readDWD.binary (RW)
`r helplink("readDWD.binary")`
```{r readDWD_binary_rw}
link <- "hourly/radolan/reproc/2017_002/bin/2017/RW2017.002_201712.tar.gz"  # 25 MB
file <- dataDWD(link, base=gridbase, joinbf=TRUE, dir=ddir, read=FALSE)
rad <- readDWD(file, exdir=tdir, selection=1:3)
pp <- project_and_plot(rad$dat, ".binary RW", rad$meta$date[1], extent="rw")
```

## readDWD.binary (SF)
`r helplink("readDWD.binary")`  
```{r readDWD_binary_sf}
link <- "/daily/radolan/historical/bin/2017/SF201712.tar.gz"           # 204 MB
file <- dataDWD(link, base=gridbase, joinbf=TRUE, dir=ddir, read=FALSE)
rad <- readDWD(file, exdir=tdir, selection=1:3) # with toraster=TRUE
pp <- project_and_plot(rad$dat, ".binary SF", rad$meta$date[1])
```

## readDWD.asc
`r helplink("readDWD.asc")` 
```{r readDWD_asc}
link <- "hourly/radolan/historical/asc/2018/RW-201809.tar" # 25 mB
file <- dataDWD(link, base=gridbase, joinbf=TRUE, dir=ddir,
                dbin=TRUE, read=FALSE) # download with mode=wb!!!
rad <- readDWD(file, selection=1:3, dividebyten=TRUE)
pp <- project_and_plot(rad, ".asc", names(rad)[1])
```
 
## readDWD.radar (RW)
`r helplink("readDWD.radar")`
```{r readDWD_radar, echo=-1}
suppressPackageStartupMessages(library(R.utils)) 
links <- indexFTP("hourly/radolan/recent/bin", base=gridbase, dir=tdir) # 0.04 MB
file <- dataDWD(links[773], base=gridbase, joinbf=TRUE, dir=tdir, read=FALSE)
rad <- readDWD(file)
pp <- project_and_plot(rad$dat, ".radar RW", rad$meta$date)
```

## readDWD.radar (RQ)
`r helplink("readDWD.radar")`
```{r readDWD_radar_RQ}
rqbase <- "ftp://opendata.dwd.de/weather/radar/radvor/rq"
links <- indexFTP("", base=rqbase, dir=tdir) # 0.07 MB
file <- dataDWD(links[17], base=rqbase, joinbf=TRUE, dir=tdir, read=FALSE)
rad <- readDWD(file)
pp <- project_and_plot(rad$dat, ".radar RQ", rad$meta$date)
```


## binary file errors
Binary files must be downloaded by `download.file` with wb=TRUE (at least on Windows, due to CRLF issues).
`download.file` will automatically do that for some file endings (like .gz, .zip).
For others, `r helplink("dataDWD")` has a dbin=TRUE option.  
If you do not use this, your plots will look partially shifted like this (and have the wrong units):
```{r wrong_binary}
url <- "ftp://ftp-cdc.dwd.de/weather/radar/radolan/rw/raa01-rw_10000-latest-dwd---bin"
rw_file <- dataDWD(url, dir=tempdir(), read=FALSE, dbin=FALSE)
rw_orig <- dwdradar::readRadarFile(rw_file)
raster::plot(raster::raster(rw_orig$dat))
```

I'm considering to set the default dbin to TRUE but need to assess the implications yet.


## PDF overview

```{r raster_pdf}
pdf("../ExampleTests/Radartests_Vign.pdf", width=6, height=6)
par(mar=c(0,0,3,5))
dummy <- lapply(pp, function(x) plotRadar(x, xlim=c(3,16), ylim=c(47, 55), 
                                        main=x@title, project=FALSE))
dev.off()
```

Open the pdf at <https://github.com/brry/rdwd/tree/master/misc/ExampleTests>


# - use case: recent time series

download & read data

```{r uc_recent_time_series_data, eval=TRUE, fig.height=3, fig.width=7}
library(rdwd)
link <- selectDWD("Potsdam", res="daily", var="kl", per="recent")
file <- dataDWD(link, read=FALSE, dir="../localdata", force=NA, overwrite=TRUE)
clim <- readDWD(file, varnames=TRUE)

str(clim)
```

plot time series

```{r uc_recent_time_series_plot, eval=TRUE, fig.height=3, fig.width=7}
par(mar=c(4,4,2,0.5), mgp=c(2.7, 0.8, 0), cex=0.8)
plot(clim[,c(2,14)], type="l", xaxt="n", las=1, main="Daily temp Potsdam")
berryFunctions::monthAxis()   ;   abline(h=0)
mtext("Source: Deutscher Wetterdienst", adj=-0.1, line=0.5, font=3)
```

# - use case: long term climate graph

```{r uc_climgraph, eval=TRUE, fig.height=3, fig.width=7, echo=-1}
par(mar=c(4,4,2,0.5), mgp=c(2.7, 0.8, 0), cex=0.8)
link <- selectDWD("Goettingen", res="monthly", var="kl", per="h")
clim <- dataDWD(link, dir="../localdata")

clim$month <- substr(clim$MESS_DATUM_BEGINN,5,6)
temp <- tapply(clim$MO_TT, clim$month, mean, na.rm=TRUE)
prec <- tapply(clim$MO_RR, clim$month, mean, na.rm=TRUE)

berryFunctions::climateGraph(temp, prec, main="Goettingen")
mtext("Source: Deutscher Wetterdienst", adj=-0.05, line=2.8, font=3)
```



# - use case: monthy gridded data

From the [monthly](ftp://ftp-cdc.dwd.de/climate_environment/CDC/grids_germany/monthly) 
folder at `r helplink("gridbase")`, we want to
download .asc.gz files for selected years and open them in R for further processing.

```{r grid_monthly_index, message=FALSE}
data("gridIndex")
index <- grep("monthly",   gridIndex, value=TRUE) # 12'295
index <- grep('precipitation', index, value=TRUE) #  1'664
index <- grep('2014|2015|2016',index, value=TRUE) #     36 (3*12)

precip_files <- dataDWD(index[6:8], base=gridbase, joinbf=TRUE, read=FALSE, 
                        dir="../localdata")
precip <- readDWD(precip_files)
```
For .asc.gz files, `r helplink("readDWD")` calls `r helplink("readDWD.raster")`.
This runs faster if called a second time due to skip=TRUE in `gunzip`.  
Now we can project and visualize with:

```{r grid_monthly_vis}
# raster::plot(precip[[1]])
precip_proj <- projectRasterDWD(precip[[1]], proj="seasonal", 
                                extent=precip[[1]]@extent)
raster::plot(precip_proj, las=1)
addBorders()
```

For further processing, we can create a raster stack from the list, which e.g. enables fast and easy indexing.
```{r grid_monthly_stack, eval=FALSE}
precip_stack <- raster::stack(precip)
```

A few projection references:

The [Beschreibung file](ftp://opendata.dwd.de/climate_environment/CDC/grids_germany/seasonal/air_temperature_max/BESCHREIBUNG_gridsgermany_seasonal_air_temperature_max_de.pdf)
leads to
<https://spatialreference.org/ref/epsg/31467/>
from which the proj4 format is used internally in `r helplink("projectRasterDWD")`, currently at 
[line 54](https://github.com/brry/rdwd/blob/master/R/projectRasterDWD.R#L54).  
Bettina Ohse reported using `dwd.projection <- '+init=epsg:31467'` instead.


# - use case: recent hourly radar files
Single RW files at <ftp://ftp-cdc.dwd.de/weather/radar/radolan/rw> should be read
with the underlying [`readRadarFile`](https://www.rdocumentation.org/packages/dwdradar/topics/readRadarFile)
that has been outsourced to `dwdradar` to keep the basic `rdwd` as lean as possible.

Since these datasets only exist for two days on the FTP Server, I'm storing them in `tempdir()`.

Please note that for projecting (see `r helplink("projectRasterDWD")`), 
the radolan extent seems to be needed, not the rw extent.

```{r recent_radar}
rw_base <- "ftp://ftp-cdc.dwd.de/weather/radar/radolan/rw"
rw_urls <- indexFTP(base=rw_base, dir=tempdir(), folder="", exclude.latest.bin=TRUE)
rw_file <- dataDWD(rw_urls[8], base=rw_base, joinbf=TRUE, dir=tempdir(), read=FALSE, dbin=TRUE)

rw_orig <- dwdradar::readRadarFile(rw_file)
str(rw_orig)
rw_proj <- projectRasterDWD(raster::raster(rw_orig$dat), extent="radolan")
raster::plot(rw_proj) # NB: is rw file, but needs radolan extent instead of rw
addBorders()
```



# - use case: longest time series in Berlin

## select station
Choose station in Berlin with longest monthly average recordings
(according to metadata, which is not always correct).
```{r uc_berlinstationselection}
ids <- findID("Berlin", exactmatch=FALSE)
head(ids)
data("metaIndex")
berlin <- metaIndex[with(metaIndex, 
              Stations_id %in% ids & res=="monthly" & var=="kl" & per=="historical"),]
berlin$ndays <- berlin$bis_datum - berlin$von_datum
berlin <- berryFunctions::sortDF(berlin, ndays)
berlin # Dahlem (FU) has data since 1719 !
```

## download and inspect data
```{r uc_berlindata, fig.height=5, fig.width=7, echo=-1}
par(mar=c(2,2,0.2,0.2))
url <- selectDWD("Berlin-Dahlem (FU)", res="monthly", var="kl", per="h")
kl <- dataDWD(url, varnames=TRUE, dir="../localdata")
plot(kl$MESS_DATUM, kl$MO_TT.Lufttemperatur, type="l", las=1) # pretty complete
```

## aggregates by month
```{r uc_berlinmonthly, fig.height=5, fig.width=7, echo=-1}
par(mar=c(1.5, 3, 2, 0.2))
monthly <- tapply(kl$MO_TT.Lufttemperatur, format(kl$MESS_DATUM,"%m"), quantile, probs=0:10/10)
monthly <- sapply(monthly, I)

plot(1, type="n", xlim=c(1,12), ylim=range(monthly), xaxt="n", las=1, 
     xlab="Vis by Berry B, github.com/brry/rdwd", ylab="monthly temperature average",
     main="Temperature variation in Berlin is highest in winter")
axis(1, 2:12-0.5, labels=FALSE)
axis(1, 1:12, substr(month.abb,1,1), tick=FALSE, line=-0.5)
for(m in 1:12) for(q in 1:5) lines(c(m,m), monthly[c(q,12-q),m], lend=1, 
                                   col=berryFunctions::addAlpha("red",0.2), lwd=5)
for(m in 1:12) points(m, mean(monthly[,m]), pch=3)
```


# - use case: Rainfall intensity depends on temperature

Clausius-Clapeyron scaling holds even for very high temperatures, 
we just don't have enough data yet to have observed the expected extreme rainfall intensities.

If quantiles are estimated by appropriately fitting a GDP and using its quantiles,
extreme rainfall intensity estimates continue to rise with air temperature.

<img src="CC_rainfall.PNG" width="600">


Code (with a much older version of `rdwd`, might not run out of the box any more):
<https://github.com/brry/prectemp/blob/master/Code_analysis.R>  
Publication:
<http://www.nat-hazards-earth-syst-sci-discuss.net/nhess-2016-183>

# - use case: Get all hourly rainfall data 2014:2016

## get the URLS of data to be downloaded

```{r hourlyrain_data_selection, warning=FALSE}
library(rdwd)
links <- selectDWD(res="daily", var="more_precip", per="hist")
length(links) # ca 5k stations - would take very long to download

# select only the relevant files:
data("metaIndex")
myIndex <- metaIndex[
  metaIndex$von_datum < as.Date("2014-01-01") &
  metaIndex$bis_datum > as.Date("2016-12-31") & metaIndex$hasfile   ,  ]
data("fileIndex")    
links <- fileIndex[
  suppressWarnings(as.numeric(fileIndex$id)) %in% myIndex$Stations_id &
  fileIndex$res=="daily" &
  fileIndex$var=="more_precip" &
  fileIndex$per=="historical"         , "path" ]  

length(links) # ca 2k elements - much better
```


## download the data

If some downloads fail (mostly because you'll get kicked off the FTP server),
you can just run the same code again and only the missing files will be downloaded.

If you really want to download 2k historical (large!) datasets, 
you might need to set `sleep` in `r helplink("dataDWD")` to a relatively high value.

For speed, we'll only work with the first 3 urls.

```{r hourlyrain_data_download, message=FALSE}
localfiles <- dataDWD(links[1:3], joinbf=TRUE, sleep=0.2, read=FALSE, dir="../localdata")
```


## read the data

2k large datasets probably is way too much for memory, so we'll use a custom reading function.
It will only select the relevant time section and rainfall column.
The latter will be named with the id extracted from the filename.

```{r hourlyrain_reading_function, message=FALSE}
readVars(localfiles[1])[,-3] # we want the RS column

read2014_2016 <- function(file, fread=TRUE, ...)
{
 out <- readDWD(file, fread=fread, ...)
 out <- out[out$MESS_DATUM > as.POSIXct(as.Date("2014-01-01")) & 
            out$MESS_DATUM < as.POSIXct(as.Date("2016-12-31"))    , ]
 out <- out[ , c("MESS_DATUM", "RS")]
 out$MESS_DATUM <- as.Date(out$MESS_DATUM) # might save some memory space...
 # Station id as column name:
 idstringloc <- unlist(gregexpr(pattern="tageswerte_RR_", file))
 idstring <- substring(file, idstringloc+14, idstringloc+18)
 colnames(out) <- c("date",  idstring)
 return(out)
}
str(read2014_2016(localfiles[1])) # test looks good
```

Now let's apply this to all our files and merge the result.

```{r hourlyrain_data_reading, message=FALSE}
library(pbapply) # progress bar for lapply loop

rain_list <- pblapply(localfiles, read2014_2016)
rain_df <- Reduce(function(...) merge(..., all=T), rain_list)
str(rain_df) # looks nice!
summary(rain_df) # 9 NAs in station 00006
```


## visual data checks

```{r hourlyrain_vis, fig.height=3, fig.width=6}
plot(rain_df$date, rain_df[,2], type="n", ylim=range(rain_df[,-1], na.rm=T), 
     las=1, xaxt="n", xlab="Date", ylab="Daily rainfall sum  [mm]")
berryFunctions::monthAxis()
for(i in 2:ncol(rain_df)) lines(rain_df$date, rain_df[,i], col=sample(colours(), size=1))

plot(rain_df[,2:4]) # correlation plot only works for a few columns!
```

Let's see the locations of our stations in an interactive map.

```{r hourlyrain_map_interactive, warning=FALSE, fig.height=3, fig.width=6}
data(geoIndex)  ;  library(leaflet) 
mygeoIndex <- geoIndex[geoIndex$id %in% as.numeric(colnames(rain_df)[-1]),]

leaflet(data=mygeoIndex) %>% addTiles() %>%
        addCircleMarkers(~lon, ~lat, popup=~display, stroke=T)
```    

For a static map with scaleBar, OSMscale works nicely but has a Java dependency, see
<https://github.com/brry/OSMscale#installation>

```{r hourlyrain_map_static, message=FALSE, fig.height=2, fig.width=6}
library(OSMscale)
pointsMap("lat", "lon", mygeoIndex, fx=2, fy=1, pargs=list(lwd=3), 
                    col="blue", zoom=5)
```    




# - use case: plot all rainfall values around a given point

## Find meteo stations around a given point

```{r rainregion_nearbyStations, message=FALSE}
m <- nearbyStations(49.211784, 9.812475, radius=30,
    res=c("daily","hourly"), var=c("precipitation","more_precip","kl"),
    mindate=as.Date("2016-05-30"), statname="Braunsbach catchment center")
# Remove duplicates. if kl and more_precip are both available, keep only more_precip:
library("berryFunctions")
m <- sortDF(m, "var")
m <- m[!duplicated(paste0(m$Stations_id, m$res)),]
m <- sortDF(m, "res")
m <- sortDF(m, "dist", decreasing=FALSE)
rownames(m) <- NULL
DT::datatable(m, options=list(pageLength=5, scrollX=TRUE))
```

Interactive map of just the meteo station locations:
```{r rainregion_interactive_map, message=FALSE, fig.height=3, fig.width=4}
library(leaflet)
m$col <- "red" ; m$col[1] <- "blue"
leaflet(m) %>% addTiles() %>%
  addCircles(lng=9.812475, lat=49.211784, radius=30e3) %>%
  addCircleMarkers(~geoLaenge, ~geoBreite, col=~col, popup=~Stationsname)
```

## Download and process data

Download and process data for the stations, get the rainfall sums of a particular day (Braunsbach flood May 2016):
```{r rainregion_download_data, message=FALSE, warning=FALSE}
prec <- dataDWD(m$url, fread=TRUE, dir="../localdata")
names(prec) <- m$Stations_id[-1]
prec29 <- sapply(prec[m$res[-1]=="daily"], function(x)
         {
         if(nrow(x)==0) return(NA)
         col <- "RS"
         if(!col %in% colnames(x)) col <- "R1"
         if(!col %in% colnames(x)) col <- "RSK"
         x[x$MESS_DATUM==as.POSIXct(as.Date("2016-05-29")), col]
         })
prec29 <- data.frame(Stations_id=names(prec29), precsum=unname(prec29))
prec29 <- merge(prec29, m[m$res=="daily",c(1,4:7,14)], sort=FALSE)
head(prec29[,-7]) # don't show url column with long urls
```

7 of the files contain no rows. readDWD warns about this (but the warnings are suppressed in this vignette).  
One example is daily/more_precip/historical/tageswerte_RR_07495_20070114_20181231_hist.zip

## Plot rainfall sum on map

For a quick look without a map, this works:
```{r rainregion_static_points, eval=FALSE}
plot(geoBreite~geoLaenge, data=m, asp=1)
textField(prec29$geoLaenge, prec29$geoBreite, prec29$precsum, col=2)
```

But it's nicer to have an actual map.
If OSMscale installation fails, go to <https://github.com/brry/OSMscale#installation>
```{r rainregion_static_map, message=FALSE, fig.height=4.1, fig.width=4}
library(OSMscale)
map <- pointsMap(geoBreite,geoLaenge, data=m, type="osm", plot=FALSE)
pp <- projectPoints("geoBreite", "geoLaenge", data=prec29, to=map$tiles[[1]]$projection)
prec29 <- cbind(prec29,pp) ; rm(pp)
pointsMap(geoBreite,geoLaenge, data=m, map=map, scale=FALSE)
textField(prec29$x, prec29$y, round(prec29$precsum), font=2, cex=1.5)
scaleBar(map, cex=1.5, type="line", y=0.82)
title(main="Rainfall sum  2016-05-29  7AM-7AM  [mm]", line=-1)
```


# - use case: map climate data to Landkreise

Shapefile of Landkreis districts:  
<https://public.opendatasoft.com/explore/dataset/landkreise-in-germany/export/>
(file size 4 MB, unzipped 10 MB)

## find available meteo stations for each district

```{r climdistrict_data_selection}
# Select monthly climate data:
data("metaIndex") ; m <- metaIndex
m <- m[m$res=="monthly" & m$var=="kl" & m$per=="recent" & m$hasfile, ]
# Transform into spatial object:
msf <- sf::st_as_sf(m, coords=c("geoLaenge", "geoBreite"), crs=4326)

# Read district shapefile, see link above:
lk <- sf::st_read("landkreise-in-germany.shp")

# intersections: list with msf rownumbers for each district:
int <- sf::st_intersects(lk, msf)
```

<https://gis.stackexchange.com/a/318629/36710>

```{r climdistrict_plot, fig.height=5}
# plot to check projection:
plot(lk[,"id_2"], reset=FALSE)
colPoints("geoLaenge", "geoBreite", "Stationshoehe", data=m, add=T, legend=F)
# berryFunctions::colPointsLegend + sf plots = set margins, see note there!
axis(1, line=-1); axis(2, line=-1, las=1)
points(m[int[[2]], c("geoLaenge", "geoBreite")], pch=16, col=2, cex=1.8)
```


## Average data per district

Running analysis for a few selected districts only to reduce computation time.  
Monthly rainfall average per Landkreis.
```{r climdistrict_data_download}
landkreis_rain <- function(lki) # LandKreisIndex (row number in lk)
{
rnr <- int[[lki]] # msf row number
if(length(rnr)<1)
  {
  warning("No rainfall data available for Landkreis ", lki, ": ", lk$name_2[lki], call.=FALSE)
  out <- data.frame(NA,NA)[FALSE,]
  colnames(out) <- c("MESS_DATUM", as.character(lk$name_2[lki]))
  return(out)
  }
urls <- selectDWD(id=m[rnr, "Stations_id"], # set dir if needed
                  res="monthly", var="kl", per="r", outvec=TRUE)
clims <- dataDWD(urls, varnames=FALSE, dir="../localdata")
if(length(urls)==1) 
  {rainmean <- clims$MO_RR 
  monthlyrain <- clims[c("MESS_DATUM", "MO_RR")]
  } else
{
monthlyrain <- lapply(seq_along(clims), function(n) 
 {
 out <- clims[[n]][c("MESS_DATUM", "MO_RR")]
 colnames(out)[2] <- names(clims)[n] # no duplicate names
 out
 })
monthlyrain <- Reduce(function(...) merge(..., by="MESS_DATUM",all=TRUE), monthlyrain)
rainmean <- rowMeans(monthlyrain[,-1], na.rm=TRUE) # check also with median, variation is huge!
}
out <- data.frame(monthlyrain[,1], rainmean)
colnames(out) <- c("MESS_DATUM", as.character(lk$name_2[lki]))
return(out)
}

rainLK <- pbapply::pblapply(c(133,277,300,389), landkreis_rain)
rainLK <- Reduce(function(...) merge(..., by="MESS_DATUM",all=TRUE), rainLK)
head(rainLK)
```



# - use case: phenology data

Located here:
ftp://opendata.dwd.de/climate_environment/CDC/observations_germany/phenology

This example uses data in the subfolder [annual/crops/hist](ftp://opendata.dwd.de/climate_environment/CDC/observations_germany/phenology/annual_reporters/crops/historical/)
```{r}
phenocrop_base <- paste0(sub("climate$", "phenology", dwdbase), 
                        "/annual_reporters/crops/historical/")
# pheno_urls <- indexFTP("", base=phenocrop_base, dir="Pheno")
kohl_url <- "PH_Jahresmelder_Landwirtschaft_Kulturpflanze_Weisskohl_1951_1990_hist.txt"# 9 MB
kohl_file <- dataDWD(base=phenocrop_base, file=kohl_url, joinbf=TRUE, 
                     dir=localtestdir(), read=FALSE) 
kohl <- read.table(kohl_file, sep=";", header=TRUE)
```



# search

rdwd description

Weather Data Germany download with R, Climate Data Germany  
Deutscher Wetterdienst R Daten download Klimastationen  
DWD Daten mit R runterladen, Wetter und Klimadaten in R  
